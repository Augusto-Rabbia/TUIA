{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "keQKyO6-Yblz",
        "r1JUG7SvYhnq",
        "RKoneUmaYsYf",
        "uaXpyVIb_Iaw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TUIA - Procesamiento de Lenguaje Natural - Unidad 6 - Cuaderno de práctica\n",
        "**Chatbots y Sistemas de Diálogo**\n",
        "<br>\n",
        "<br>\n",
        "**Docente teoría**:<br>\n",
        "Teoría: MANSON, Juan Pablo    jpmanson@gmail.com [@juanpablomanson](https://twitter.com/juanpablomanson)<br>\n",
        "[LinkedIN](https://www.linkedin.com/in/juanpablomanson/)<br><br>"
      ],
      "metadata": {
        "id": "Kw_OR937bDap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot básico basado en reglas"
      ],
      "metadata": {
        "id": "keQKyO6-Yblz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wqghn7KVguN",
        "outputId": "323e781a-b473-41e5-8226-919238250cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Buenas noches! Soy tu asistente virtual.\n",
            "Puedo ayudarte con información sobre nuestros productos y precios.\n",
            "Escribe 'salir' para terminar la conversación.\n",
            "\n",
            "Tú: hola\n",
            "Chatbot: ¡Bienvenido! ¿En qué puedo asistirte?\n",
            "Tú: quiero una laptop\n",
            "Chatbot: Tenemos laptop. Laptop de última generación. ¿Te gustaría saber el precio?\n",
            "Tú: si, quiero saber el precio de la laptop\n",
            "Chatbot: El laptop cuesta $800. Laptop de última generación\n",
            "\n",
            "Chatbot: Programa terminado por el usuario.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# Base de conocimiento del chatbot\n",
        "knowledge_base = {\n",
        "    'saludos': {\n",
        "        'patrones': [\n",
        "            r'\\b(hola|hey|buenos días|buenas tardes|buenas noches|hi|qué tal)\\b'\n",
        "        ],\n",
        "        'respuestas': [\n",
        "            '¡Hola! ¿Cómo puedo ayudarte?',\n",
        "            '¡Bienvenido! ¿En qué puedo asistirte?',\n",
        "            '¡Hola! Estoy aquí para ayudarte'\n",
        "        ]\n",
        "    },\n",
        "    'despedidas': {\n",
        "        'patrones': [\n",
        "            r'\\b(adiós|chau|hasta luego|nos vemos|bye)\\b'\n",
        "        ],\n",
        "        'respuestas': [\n",
        "            '¡Hasta luego! Que tengas un buen día',\n",
        "            '¡Adiós! Gracias por tu visita',\n",
        "            '¡Nos vemos pronto!'\n",
        "        ]\n",
        "    },\n",
        "    'agradecimientos': {\n",
        "        'patrones': [\n",
        "            r'\\b(gracias|te agradezco|muchas gracias)\\b'\n",
        "        ],\n",
        "        'respuestas': [\n",
        "            '¡De nada! Estoy para ayudar',\n",
        "            'Es un placer poder ayudarte',\n",
        "            'No hay de qué. ¿Necesitas algo más?'\n",
        "        ]\n",
        "    },\n",
        "    'preguntas_estado': {\n",
        "        'patrones': [\n",
        "            r'\\b(cómo estás|qué tal estás|cómo te encuentras)\\b'\n",
        "        ],\n",
        "        'respuestas': [\n",
        "            'Muy bien, ¡gracias por preguntar! ¿En qué puedo ayudarte?',\n",
        "            'Excelente y listo para ayudarte. ¿Qué necesitas?',\n",
        "            '¡Todo bien! ¿Cómo puedo asistirte hoy?'\n",
        "        ]\n",
        "    },\n",
        "    'productos': {\n",
        "        'patrones': [\n",
        "            r'\\b(qué productos|qué vendés|qué tienen|qué ofrecen)\\b'\n",
        "        ],\n",
        "        'respuestas': [\n",
        "            'Tenemos una amplia variedad de productos: laptops, smartphones, tablets y accesorios',\n",
        "            'Ofrecemos productos electrónicos como computadoras, celulares y tablets',\n",
        "            'Vendemos todo tipo de dispositivos electrónicos y sus accesorios'\n",
        "        ]\n",
        "    },\n",
        "    'precios': {\n",
        "        'patrones': [\n",
        "            r'\\b(cuánto cuesta|precio|valor|cuánto vale)\\b'\n",
        "        ],\n",
        "        'respuestas': [\n",
        "            'Los precios varían según el producto específico. ¿Qué producto te interesa?',\n",
        "            'Para darte el precio exacto, ¿podrías decirme qué producto te interesa?',\n",
        "            '¿Qué producto específico te gustaría consultar?'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Productos y sus precios\n",
        "productos = {\n",
        "    'laptop': {'precio': 800, 'descripcion': 'Laptop de última generación'},\n",
        "    'smartphone': {'precio': 500, 'descripcion': 'Smartphone con gran cámara'},\n",
        "    'tablet': {'precio': 300, 'descripcion': 'Tablet perfecta para entretenimiento'},\n",
        "    'auriculares': {'precio': 50, 'descripcion': 'Auriculares inalámbricos'},\n",
        "}\n",
        "\n",
        "def get_time_based_greeting():\n",
        "    \"\"\"Retorna un saludo basado en la hora del día\"\"\"\n",
        "    hora = datetime.now().hour\n",
        "    if 5 <= hora < 12:\n",
        "        return \"¡Buenos días!\"\n",
        "    elif 12 <= hora < 20:\n",
        "        return \"¡Buenas tardes!\"\n",
        "    else:\n",
        "        return \"¡Buenas noches!\"\n",
        "\n",
        "def buscar_producto(mensaje):\n",
        "    \"\"\"Busca menciones de productos en el mensaje\"\"\"\n",
        "    for producto in productos.keys():\n",
        "        if producto in mensaje.lower():\n",
        "            return producto\n",
        "    return None\n",
        "\n",
        "def procesar_mensaje(mensaje):\n",
        "    \"\"\"Procesa el mensaje del usuario y retorna una respuesta apropiada\"\"\"\n",
        "    mensaje = mensaje.lower()\n",
        "\n",
        "    # Buscar producto específico\n",
        "    producto = buscar_producto(mensaje)\n",
        "    if producto:\n",
        "        if 'precio' in mensaje or 'cuesta' in mensaje or 'valor' in mensaje:\n",
        "            return f\"El {producto} cuesta ${productos[producto]['precio']}. {productos[producto]['descripcion']}\"\n",
        "        return f\"Tenemos {producto}. {productos[producto]['descripcion']}. ¿Te gustaría saber el precio?\"\n",
        "\n",
        "    # Buscar coincidencias en la base de conocimiento\n",
        "    for categoria, datos in knowledge_base.items():\n",
        "        for patron in datos['patrones']:\n",
        "            if re.search(patron, mensaje):\n",
        "                return random.choice(datos['respuestas'])\n",
        "\n",
        "    # Si no se encuentra coincidencia\n",
        "    return \"No estoy seguro de entender. ¿Podrías reformular tu pregunta?\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal del chatbot\"\"\"\n",
        "    print(f\"{get_time_based_greeting()} Soy tu asistente virtual.\")\n",
        "    print(\"Puedo ayudarte con información sobre nuestros productos y precios.\")\n",
        "    print(\"Escribe 'salir' para terminar la conversación.\\n\")\n",
        "\n",
        "    historial = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Obtener input del usuario\n",
        "            entrada_usuario = input(\"Tú: \").strip()\n",
        "\n",
        "            # Guardar en historial\n",
        "            historial.append((\"usuario\", entrada_usuario))\n",
        "\n",
        "            # Verificar si el usuario quiere salir\n",
        "            if entrada_usuario.lower() == 'salir':\n",
        "                print(\"Chatbot: ¡Gracias por tu visita! ¡Hasta pronto!\")\n",
        "                break\n",
        "\n",
        "            # Procesar el mensaje y obtener respuesta\n",
        "            respuesta = procesar_mensaje(entrada_usuario)\n",
        "            print(\"Chatbot:\", respuesta)\n",
        "\n",
        "            # Guardar respuesta en historial\n",
        "            historial.append((\"bot\", respuesta))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nChatbot: Programa terminado por el usuario.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Chatbot: Lo siento, ha ocurrido un error. Por favor, intenta de nuevo.\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot basado en reglas (mejorado)"
      ],
      "metadata": {
        "id": "r1JUG7SvYhnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%capture\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lees_9ENXNNH",
        "outputId": "ec0bc706-50d4-48fe-855b-063874c083eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%capture` not found (But cell magic `%%capture` exists, did you mean that instead?).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplos de Prompts para Chatbot \"ElectroMax\"<br><br>\n",
        "**Consultas de Precios**\n",
        "\n",
        "\"¿Cuánto cuesta la heladera?\"<br>\n",
        "\"¿Qué precio tiene el lavarropas?\"<br>\n",
        "\"¿Me podés decir el valor de la licuadora?\"<br>\n",
        "\"¿A cuánto está la cafetera?\"<br>\n",
        "\"¿Cuál es el precio final del microondas con descuento?\"<br>\n",
        "\n",
        "**Consultas de Características**\n",
        "\n",
        "\"¿Qué características tiene la heladera?\"<br>\n",
        "\"Contame las especificaciones del lavarropas\"<br>\n",
        "\"¿Qué funciones tiene el microondas?\"<br>\n",
        "\"Dame los detalles de la aspiradora\"<br>\n",
        "\"¿Cuáles son las características de la cafetera?\"<br>\n",
        "\n",
        "**Consultas de Stock**\n",
        "\n",
        "\"¿Tienen heladera en stock?\"<br>\n",
        "\"¿Hay lavarropas disponible?\"<br>\n",
        "\"¿Me podés decir si tienen licuadora?\"<br>\n",
        "\"¿Cuántas cafeteras les quedan?\"<br>\n",
        "\"¿Hay stock de aspiradora?\"<br>\n",
        "\n",
        "**Consultas de Promociones**\n",
        "\n",
        "\"¿Qué promociones tienen?\"<br>\n",
        "\"¿Hay algún descuento en heladeras?\"<br>\n",
        "\"¿Tienen ofertas este fin de semana?\"<br>\n",
        "\"¿Hay rebajas en electrodomésticos?\"<br>\n",
        "\"¿Cuáles son las promociones vigentes?\"<br>\n",
        "\n",
        "**Comparaciones**\n",
        "\n",
        "\"¿Cuál es mejor, la heladera o el lavarropas?\"<br>\n",
        "\"Comparame la licuadora con la cafetera\"<br>\n",
        "\"¿Qué diferencia hay entre el microondas y la cafetera?\"<br>\n",
        "\"Quiero comparar la aspiradora y el lavarropas\"<br>\n",
        "\"¿Me podés comparar la heladera con el microondas?\"\n",
        "\n",
        "**Consultas Combinadas**\n",
        "\n",
        "\"¿Qué características tiene la heladera y cuánto cuesta?\"<br>\n",
        "\"¿Hay stock de lavarropas y qué precio tiene?\"<br>\n",
        "\"Quiero saber el precio y las funciones del microondas\"<br>\n",
        "\"¿Me podés decir el precio y si hay stock de licuadora?\"<br>\n",
        "\"¿Qué características tiene la aspiradora y hay alguna promoción?\"<br>\n",
        "\n",
        "**Consultas Múltiples Productos**\n",
        "\n",
        "\"Quiero información de la heladera y el lavarropas\"<br>\n",
        "\"¿Me podés decir los precios de la licuadora y la cafetera?\"<br>\n",
        "\"¿Qué características tienen el microondas y la aspiradora?\"<br>\n",
        "\"¿Hay stock de heladera y lavarropas?\"<br>\n",
        "\"Comparame la licuadora, cafetera y microondas\"<br>\n",
        "\n",
        "**Preguntas Generales**\n",
        "\n",
        "\"¿Qué productos tienen?\"<br>\n",
        "\"¿Me podés mostrar las ofertas?\"<br>\n",
        "\"¿Cuáles son los electrodomésticos más vendidos?\"<br>\n",
        "\"¿Qué me recomendás para la cocina?\"<br>\n",
        "\"¿Tienen servicio de entrega?\"<br>\n",
        "\n",
        "**Ejemplos de Diálogo Natural**\n",
        "\n",
        "\"Hola, estoy buscando una heladera nueva, ¿qué me recomendás?\"<br>\n",
        "\"Necesito renovar los electrodomésticos de mi cocina\"<br>\n",
        "\"Me interesa comprar un lavarropas, ¿qué opciones tienen?\"<br>\n",
        "\"Estoy por mudarme y necesito equipar mi casa\"<br>\n",
        "\"¿Me podés ayudar a elegir entre estos productos?\"<br>"
      ],
      "metadata": {
        "id": "eRdqLoTtKYGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download es_core_news_md\n",
        "\n",
        "import spacy\n",
        "import random\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Cargar el modelo preentrenado de spaCy\n",
        "nlp = spacy.load('es_core_news_md')\n",
        "\n",
        "# Información ampliada sobre productos y promociones\n",
        "productos = {\n",
        "    'lavarropas': {\n",
        "        'precio': 300,\n",
        "        'descuento': 10,\n",
        "        'características': ['6kg de capacidad', 'Eficiencia energética A++', 'Multiple programas de lavado'],\n",
        "        'stock': 5\n",
        "    },\n",
        "    'aspiradora': {\n",
        "        'precio': 200,\n",
        "        'descuento': 5,\n",
        "        'características': ['Sin bolsa', 'Filtro HEPA', 'Potencia 2000W'],\n",
        "        'stock': 8\n",
        "    },\n",
        "    'heladera': {\n",
        "        'precio': 1000,\n",
        "        'descuento': 20,\n",
        "        'características': ['No frost', '500L de capacidad', 'Dispensador de agua'],\n",
        "        'stock': 3\n",
        "    },\n",
        "    'microondas': {\n",
        "        'precio': 150,\n",
        "        'descuento': 15,\n",
        "        'características': ['30L de capacidad', 'Grill integrado', 'Panel digital'],\n",
        "        'stock': 10\n",
        "    },\n",
        "    'licuadora': {\n",
        "        'precio': 50,\n",
        "        'descuento': 5,\n",
        "        'características': ['Vaso de vidrio', '5 velocidades', '600W de potencia'],\n",
        "        'stock': 15\n",
        "    },\n",
        "    'cafetera': {\n",
        "        'precio': 40,\n",
        "        'descuento': 5,\n",
        "        'características': ['Capacidad 1.5L', 'Sistema antigoteo', 'Filtro permanente'],\n",
        "        'stock': 12\n",
        "    }\n",
        "}\n",
        "\n",
        "# Promociones con fechas de validez\n",
        "promociones = [\n",
        "    {\n",
        "        \"mensaje\": \"¡Este fin de semana todos los productos con 15% de descuento!\",\n",
        "        \"validez\": \"hasta el domingo\",\n",
        "        \"descuento\": 15,\n",
        "        \"productos_aplicables\": \"todos\"\n",
        "    },\n",
        "    {\n",
        "        \"mensaje\": \"Por la compra de una heladera, lleva una licuadora con 50% de descuento\",\n",
        "        \"validez\": \"próximos 30 días\",\n",
        "        \"descuento\": 50,\n",
        "        \"productos_aplicables\": [\"heladera\", \"licuadora\"]\n",
        "    },\n",
        "    {\n",
        "        \"mensaje\": \"Compra una lavadora y secadora en combo y ahorra $100\",\n",
        "        \"validez\": \"stock disponible\",\n",
        "        \"descuento_fijo\": 100,\n",
        "        \"productos_aplicables\": [\"lavarropas\", \"secadora\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Palabras clave para diferentes intenciones\n",
        "intenciones = {\n",
        "    'precio': ['precio', 'costo', 'vale', 'valor', 'cuánto'],\n",
        "    'caracteristicas': ['características', 'especificaciones', 'detalles', 'funciones'],\n",
        "    'stock': ['stock', 'disponible', 'hay', 'tienen'],\n",
        "    'promocion': ['promoción', 'descuento', 'oferta', 'rebaja'],\n",
        "    'comparacion': ['mejor', 'diferencia', 'comparar', 'versus', 'vs']\n",
        "}\n",
        "\n",
        "def detectar_intencion(message):\n",
        "    \"\"\"Detecta la intención principal del mensaje del usuario\"\"\"\n",
        "    message = message.lower()\n",
        "    intenciones_detectadas = []\n",
        "\n",
        "    for intencion, palabras_clave in intenciones.items():\n",
        "        if any(palabra in message for palabra in palabras_clave):\n",
        "            intenciones_detectadas.append(intencion)\n",
        "\n",
        "    return intenciones_detectadas if intenciones_detectadas else ['general']\n",
        "\n",
        "def calcular_precio_final(producto):\n",
        "    \"\"\"Calcula el precio final con descuento\"\"\"\n",
        "    precio_base = productos[producto]['precio']\n",
        "    descuento = productos[producto]['descuento']\n",
        "    precio_final = precio_base * (1 - descuento/100)\n",
        "    return precio_final\n",
        "\n",
        "def buscar_producto(message):\n",
        "    \"\"\"Busca productos mencionados en el mensaje usando lematización\"\"\"\n",
        "    doc = nlp(message.lower())\n",
        "    lemas = [token.lemma_ for token in doc]\n",
        "    productos_encontrados = []\n",
        "\n",
        "    for producto in productos:\n",
        "        if producto in lemas:\n",
        "            productos_encontrados.append(producto)\n",
        "\n",
        "    return productos_encontrados\n",
        "\n",
        "def generar_respuesta_producto(producto, intenciones):\n",
        "    \"\"\"Genera una respuesta detallada según el producto y las intenciones detectadas\"\"\"\n",
        "    respuesta = []\n",
        "    datos = productos[producto]\n",
        "\n",
        "    if 'precio' in intenciones:\n",
        "        precio_final = calcular_precio_final(producto)\n",
        "        respuesta.append(f\"El {producto} tiene un precio de ${datos['precio']}, con un {datos['descuento']}% de descuento. \"\n",
        "                        f\"Precio final: ${precio_final:.2f}\")\n",
        "\n",
        "    if 'caracteristicas' in intenciones:\n",
        "        respuesta.append(f\"Características principales: {', '.join(datos['características'])}\")\n",
        "\n",
        "    if 'stock' in intenciones:\n",
        "        respuesta.append(f\"Actualmente tenemos {datos['stock']} unidades disponibles\")\n",
        "\n",
        "    if 'general' in intenciones:\n",
        "        precio_final = calcular_precio_final(producto)\n",
        "        respuesta.append(f\"El {producto} está disponible por ${precio_final:.2f} (incluye {datos['descuento']}% de descuento). \"\n",
        "                        f\"Tenemos {datos['stock']} unidades en stock.\")\n",
        "\n",
        "    return \" \".join(respuesta)\n",
        "\n",
        "def comparar_productos(productos_lista):\n",
        "    \"\"\"Compara dos o más productos\"\"\"\n",
        "    if len(productos_lista) < 2:\n",
        "        return \"Por favor, menciona al menos dos productos para comparar.\"\n",
        "\n",
        "    comparacion = \"Comparación de productos:\\n\\n\"\n",
        "    for producto in productos_lista:\n",
        "        datos = productos[producto]\n",
        "        precio_final = calcular_precio_final(producto)\n",
        "        comparacion += f\"{producto.capitalize()}:\\n\"\n",
        "        comparacion += f\"- Precio final: ${precio_final:.2f}\\n\"\n",
        "        comparacion += f\"- Características: {', '.join(datos['características'])}\\n\"\n",
        "        comparacion += f\"- Stock disponible: {datos['stock']}\\n\\n\"\n",
        "\n",
        "    return comparacion\n",
        "\n",
        "def process_message(message):\n",
        "    \"\"\"Procesa el mensaje del usuario y genera una respuesta apropiada\"\"\"\n",
        "    # Detectar intenciones\n",
        "    intenciones_detectadas = detectar_intencion(message)\n",
        "\n",
        "    # Buscar productos mencionados\n",
        "    productos_encontrados = buscar_producto(message)\n",
        "\n",
        "    # Si no se encontraron productos\n",
        "    if not productos_encontrados:\n",
        "        if 'promocion' in intenciones_detectadas:\n",
        "            promo = random.choice(promociones)\n",
        "            return f\"¡Tenemos esta promoción especial: {promo['mensaje']} ({promo['validez']})\"\n",
        "        return \"No he identificado ningún producto específico. ¿Puedes ser más específico sobre qué producto te interesa?\"\n",
        "\n",
        "    # Si se encontró más de un producto y se detecta intención de comparación\n",
        "    if len(productos_encontrados) > 1 and ('comparacion' in intenciones_detectadas):\n",
        "        return comparar_productos(productos_encontrados)\n",
        "\n",
        "    # Generar respuesta para cada producto encontrado\n",
        "    respuestas = []\n",
        "    for producto in productos_encontrados:\n",
        "        respuesta = generar_respuesta_producto(producto, intenciones_detectadas)\n",
        "        respuestas.append(respuesta)\n",
        "\n",
        "    return \"\\n\".join(respuestas)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal para ejecutar el chatbot\"\"\"\n",
        "    print(\"¡Hola! Soy el chatbot de ElectroMax. Puedo ayudarte con:\")\n",
        "    print(\"- Información de productos y precios\")\n",
        "    print(\"- Características y especificaciones\")\n",
        "    print(\"- Disponibilidad y stock\")\n",
        "    print(\"- Promociones vigentes\")\n",
        "    print(\"- Comparación entre productos\")\n",
        "    print(\"\\nEscribe 'salir' para terminar la conversación.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nTú: \")\n",
        "            if user_input.lower() == 'salir':\n",
        "                print(\"Chatbot: ¡Gracias por visitar ElectroMax! ¡Hasta pronto!\")\n",
        "                break\n",
        "\n",
        "            bot_response = process_message(user_input)\n",
        "            print(\"\\nChatbot:\", bot_response)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nChatbot: Sesión terminada por el usuario. ¡Hasta pronto!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nChatbot: Lo siento, ha ocurrido un error. Por favor, intenta de nuevo.\")\n",
        "            continue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF54GGNaXFtJ",
        "outputId": "addd5f5e-fe58-4620-fbcb-d6295c2eabc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Hola! Soy el chatbot de ElectroMax. Puedo ayudarte con:\n",
            "- Información de productos y precios\n",
            "- Características y especificaciones\n",
            "- Disponibilidad y stock\n",
            "- Promociones vigentes\n",
            "- Comparación entre productos\n",
            "\n",
            "Escribe 'salir' para terminar la conversación.\n",
            "\n",
            "Tú: ¿Qué características tienen el microondas y la aspiradora?\n",
            "\n",
            "Chatbot: Características principales: Sin bolsa, Filtro HEPA, Potencia 2000W Actualmente tenemos 8 unidades disponibles\n",
            "Características principales: 30L de capacidad, Grill integrado, Panel digital Actualmente tenemos 10 unidades disponibles\n",
            "\n",
            "Tú: ¿Cuál es el precio final del microondas con descuento?\n",
            "\n",
            "Chatbot: El microondas tiene un precio de $150, con un 15% de descuento. Precio final: $127.50\n",
            "\n",
            "Tú: ¿Qué funciones tiene el microondas?\n",
            "\n",
            "Chatbot: Características principales: 30L de capacidad, Grill integrado, Panel digital\n",
            "\n",
            "Tú: ¿Tienen heladera en stock?\n",
            "\n",
            "Chatbot: Actualmente tenemos 3 unidades disponibles\n",
            "\n",
            "Tú: ¿Tienen ofertas este fin de semana?\n",
            "\n",
            "Chatbot: ¡Tenemos esta promoción especial: Por la compra de una heladera, lleva una licuadora con 50% de descuento (próximos 30 días)\n",
            "\n",
            "Tú: ¿Qué diferencia hay entre el microondas y la cafetera?\n",
            "\n",
            "Chatbot: Actualmente tenemos 10 unidades disponibles\n",
            "\n",
            "Tú: ¿Qué características tiene la heladera y cuánto cuesta?\n",
            "\n",
            "Chatbot: El heladera tiene un precio de $1000, con un 20% de descuento. Precio final: $800.00 Características principales: No frost, 500L de capacidad, Dispensador de agua\n",
            "\n",
            "Tú: ¿Me podés decir los precios de la licuadora y la cafetera?\n",
            "\n",
            "Chatbot: El licuadora tiene un precio de $50, con un 5% de descuento. Precio final: $47.50\n",
            "\n",
            "Tú: ¿Hay stock de heladera y lavarropas?\n",
            "\n",
            "Chatbot: Actualmente tenemos 3 unidades disponibles\n",
            "\n",
            "Tú: ¿Cuáles son los electrodomésticos más vendidos?\n",
            "\n",
            "Chatbot: No he identificado ningún producto específico. ¿Puedes ser más específico sobre qué producto te interesa?\n",
            "\n",
            "Tú: ¿Tienen servicio de entrega?\n",
            "\n",
            "Chatbot: No he identificado ningún producto específico. ¿Puedes ser más específico sobre qué producto te interesa?\n",
            "\n",
            "Tú: Hola, estoy buscando una heladera nueva, ¿qué me recomendás?\n",
            "\n",
            "Chatbot: El heladera está disponible por $800.00 (incluye 20% de descuento). Tenemos 3 unidades en stock.\n",
            "\n",
            "Chatbot: Sesión terminada por el usuario. ¡Hasta pronto!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot basado en LLM"
      ],
      "metadata": {
        "id": "RKoneUmaYsYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install jinja2 python-decouple requests"
      ],
      "metadata": {
        "id": "-sJXDQMUgira"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from decouple import config\n",
        "from jinja2 import Template\n",
        "from google.colab import userdata\n",
        "\n",
        "def zephyr_chat_template(messages, add_generation_prompt=True):\n",
        "    # Definir la plantilla Jinja\n",
        "    template_str  = \"{% for message in messages %}\"\n",
        "    template_str += \"{% if message['role'] == 'user' %}\"\n",
        "    template_str += \"<|user|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'assistant' %}\"\n",
        "    template_str += \"<|assistant|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'system' %}\"\n",
        "    template_str += \"<|system|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% else %}\"\n",
        "    template_str += \"<|unknown|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "    template_str += \"{% endfor %}\"\n",
        "    template_str += \"{% if add_generation_prompt %}\"\n",
        "    template_str += \"<|assistant|>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "\n",
        "    # Crear un objeto de plantilla con la cadena de plantilla\n",
        "    template = Template(template_str)\n",
        "\n",
        "    # Renderizar la plantilla con los mensajes proporcionados\n",
        "    return template.render(messages=messages, add_generation_prompt=add_generation_prompt)\n",
        "\n",
        "# Tu clave API de Hugging Face\n",
        "api_key = config('HF_TOKEN', userdata.get('HF_TOKEN'))\n",
        "\n",
        "# URL de la API de Hugging Face para la generación de texto\n",
        "api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Cabeceras para la solicitud\n",
        "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "# Preparamos el prompt incluyendo el rol del sistema y el texto inicial\n",
        "chat_prompt = [{\"role\": \"system\", \"content\": \"Eres un escritor de historias de misterio.\"},\n",
        " {\"role\": \"user\", \"content\": \"Completar la frase: El cielo está\"},\n",
        "         {\"role\": \"assistant\", \"content\": \"\\\"El cielo está lleno de estrellas brillantes y el aire está frío y limpio en esta noche de luna llena, lo que hace que el misterioso silencio del campo se amplifique aún más. Me siento como un detective de Sherlock Holmes, observando y analizando cada detalle en busca de alguna clave oculta que pueda ayudarme a resolver el misterio que se avecina.\\\"\"},\n",
        "               {\"role\": \"user\", \"content\": \"¿Cómo dijiste que está el aire?\"}]\n",
        "\n",
        "prompt_with_template = zephyr_chat_template(chat_prompt)\n",
        "\n",
        "print(f\"Nuestro prompt:\\n{prompt_with_template}\")\n",
        "\n",
        "# Datos para enviar en la solicitud POST\n",
        "# Sobre los parámetros: https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "data = {\n",
        "    \"inputs\": prompt_with_template,\n",
        "    \"parameters\": {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_k\": 50,\n",
        "        \"top_p\": 0.95\n",
        "    }\n",
        "}\n",
        "\n",
        "# Realizamos la solicitud POST\n",
        "response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "print(\"\\nRespuesta del modelo:\")\n",
        "for idx, txt in enumerate(response.json()):\n",
        "  print(txt['generated_text'])"
      ],
      "metadata": {
        "id": "PB581HBTVisr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c225f0a6-8901-4946-a51f-541c713e668b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuestro prompt:\n",
            "<|system|>Eres un escritor de historias de misterio.</s>\n",
            "<|user|>Completar la frase: El cielo está</s>\n",
            "<|assistant|>\"El cielo está lleno de estrellas brillantes y el aire está frío y limpio en esta noche de luna llena, lo que hace que el misterioso silencio del campo se amplifique aún más. Me siento como un detective de Sherlock Holmes, observando y analizando cada detalle en busca de alguna clave oculta que pueda ayudarme a resolver el misterio que se avecina.\"</s>\n",
            "<|user|>¿Cómo dijiste que está el aire?</s>\n",
            "<|assistant|>\n",
            "\n",
            "\n",
            "Respuesta del modelo:\n",
            "<|system|>Eres un escritor de historias de misterio.</s>\n",
            "<|user|>Completar la frase: El cielo está</s>\n",
            "<|assistant|>\"El cielo está lleno de estrellas brillantes y el aire está frío y limpio en esta noche de luna llena, lo que hace que el misterioso silencio del campo se amplifique aún más. Me siento como un detective de Sherlock Holmes, observando y analizando cada detalle en busca de alguna clave oculta que pueda ayudarme a resolver el misterio que se avecina.\"</s>\n",
            "<|user|>¿Cómo dijiste que está el aire?</s>\n",
            "<|assistant|>\n",
            "\"El aire está frío y limpio en esta noche de luna llena.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot basado en LLM con UI"
      ],
      "metadata": {
        "id": "uaXpyVIb_Iaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí incorporamos una interfaz más amigable y el manejo de memoria en el chat, para llevar una conversación, recordando las interacciones anteriores."
      ],
      "metadata": {
        "id": "O5bt-jrmkCaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio jinja2 python-decouple llm_templates"
      ],
      "metadata": {
        "id": "CUrWF1uJ7g3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "from decouple import config\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict\n",
        "from llm_templates import Formatter, Conversation\n",
        "\n",
        "class ZephyrChat:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the ZephyrChat with API configuration\"\"\"\n",
        "        self.api_key = config('HF_TOKEN', userdata.get('HF_TOKEN'))\n",
        "        self.api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
        "        self.formatter = Formatter(huggingface_api_key=self.api_key)\n",
        "        # Mantener el historial como una variable de clase\n",
        "        self.conversation_history = [\n",
        "            {\"role\": \"system\", \"content\": \"Tu nombre es Juan y eres un asistente virtual con conocimiento general.\"}\n",
        "        ]\n",
        "\n",
        "    def format_prompt(self, messages: List[Dict[str, str]], add_generation_prompt: bool = True) -> str:\n",
        "        \"\"\"Format the conversation using llm_templates correctly\"\"\"\n",
        "        try:\n",
        "            # Asegurarse de usar todo el historial de la conversación\n",
        "            conversation = Conversation(model='zephyr', messages=messages)\n",
        "            formatted_prompt = self.formatter.render(\n",
        "                conversation,\n",
        "                add_assistant_prompt=add_generation_prompt\n",
        "            )\n",
        "            return formatted_prompt\n",
        "        except Exception as e:\n",
        "            print(f\"Error formatting prompt: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"Generate a response from the model using the full conversation history\"\"\"\n",
        "        # Agregar el nuevo mensaje del usuario al historial\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # Usar todo el historial para generar el prompt\n",
        "        formatted_prompt = self.format_prompt(self.conversation_history)\n",
        "\n",
        "        data = {\n",
        "            \"inputs\": formatted_prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": 256,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.api_url,\n",
        "                headers=self.headers,\n",
        "                json=data,\n",
        "                timeout=30\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            if isinstance(result, list) and result:\n",
        "                response_text = result[0]['generated_text']\n",
        "                # Extraer la última respuesta del asistente\n",
        "                last_response = response_text.split(\"<|assistant|>\")[-1].strip()\n",
        "                # Agregar la respuesta al historial\n",
        "                self.conversation_history.append({\"role\": \"assistant\", \"content\": last_response})\n",
        "                return last_response\n",
        "\n",
        "            return \"Lo siento, no pude generar una respuesta.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "# Mantener una instancia global del chatbot para preservar el historial entre interacciones\n",
        "chatbot_instance = ZephyrChat()\n",
        "\n",
        "def chat_function(message, history):\n",
        "    \"\"\"Handle chat interactions using the persistent chatbot instance\"\"\"\n",
        "    global chatbot_instance\n",
        "\n",
        "    # Generar respuesta usando la instancia persistente\n",
        "    response = chatbot_instance.generate_response(message)\n",
        "\n",
        "    # Devolver la respuesta y actualizar el historial visual\n",
        "    return \"\", history + [(message, response)]\n",
        "\n",
        "def clear_history():\n",
        "    \"\"\"Reset the chatbot instance and clear the conversation history\"\"\"\n",
        "    global chatbot_instance\n",
        "    chatbot_instance = ZephyrChat()\n",
        "    return None\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create the Gradio interface with improved history management\"\"\"\n",
        "    with gr.Blocks(css=\"#chatbot {height: 400px} .overflow-y-auto {height: 400px}\") as demo:\n",
        "        chatbot = gr.Chatbot(\n",
        "            [],\n",
        "            elem_id=\"chatbot\",\n",
        "            bubble_full_width=False,\n",
        "            height=400\n",
        "        )\n",
        "        msg = gr.Textbox(\n",
        "            label=\"Mensaje\",\n",
        "            placeholder=\"Escribe tu mensaje aquí...\",\n",
        "            show_label=False\n",
        "        )\n",
        "        clear = gr.Button(\"Limpiar\")\n",
        "\n",
        "        msg.submit(chat_function, [msg, chatbot], [msg, chatbot])\n",
        "        clear.click(clear_history, None, chatbot, queue=False)\n",
        "\n",
        "    return demo\n",
        "\n",
        "demo = create_gradio_interface()\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "8huYLRJ27hU7",
        "outputId": "c3097379-2356-449d-f05c-0a23bdddeff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ff3b6544636c105f03.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff3b6544636c105f03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhZp0A-17tox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}